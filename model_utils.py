# -*- coding: utf-8 -*-
"""model_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AddlExwBmqRYzKfWWORu8KGszrNdXNjC
"""

import pickle
import numpy as np
from tensorflow.keras.models import load_model

# Paths (adjust as needed)
MODEL_PATH_AUDIO = 'full_audio_metadata_model.h5'
MODEL_PATH_TEXT = 'text_model.h5'

ENCODERS = {
    'actor': 'actor_encoder.pkl',
    'emotion': 'emotion_encoder.pkl',
    'intensity': 'intensity_encoder.pkl',
    'modality': 'modality_encoder.pkl',
    'repetition': 'repetition_encoder.pkl',
    'statement': 'statement_encoder.pkl',
    'vocal': 'vocal_encoder.pkl',
}

TEXT_ENCODERS = {
    'tokenizer': 'tokenizer.pkl',
    'label_encoder': 'label_encoder_text.pkl',
}

# ---------- Load Models and Encoders ----------

def load_pickle(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

def load_all_encoders():
    encoders = {key: load_pickle(path) for key, path in ENCODERS.items()}
    text_encoders = {key: load_pickle(path) for key, path in TEXT_ENCODERS.items()}
    return encoders, text_encoders

def load_models():
    audio_model = load_model(MODEL_PATH_AUDIO)
    text_model = load_model(MODEL_PATH_TEXT)
    return audio_model, text_model

# ---------- Inference Utilities (to be implemented separately) ----------

def preprocess_audio_input(audio_path, encoders):
    """
    Extract RAVDESS metadata from filename and encode using encoders.
    """
    import os
    filename = os.path.basename(audio_path).split('.')[0]
    parts = filename.split('-')
    metadata = {
        'modality': parts[1],
        'vocal': parts[2],
        'emotion': parts[3],
        'intensity': parts[4],
        'statement': parts[5],
        'repetition': parts[6],
        'actor': parts[7],
    }

    features = []
    for key in ['actor', 'modality', 'vocal', 'emotion', 'intensity', 'statement', 'repetition']:
        encoded = encoders[key].transform([int(metadata[key])])[0]
        features.append(encoded)

    return np.array(features).reshape(1, -1)

def preprocess_text_input(text, tokenizer, max_len=100):
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    sequence = tokenizer.texts_to_sequences([text])
    return pad_sequences(sequence, maxlen=max_len)

def predict_audio(audio_path, model, encoders):
    features = preprocess_audio_input(audio_path, encoders)
    pred = model.predict(features)
    return pred

def predict_text(text, model, tokenizer, label_encoder):
    input_seq = preprocess_text_input(text, tokenizer)
    pred = model.predict(input_seq)
    label = label_encoder.inverse_transform([np.argmax(pred)])
    return label[0], pred

# ---------- Main Unified Prediction ----------

def predict_multimodal(audio_path, text, audio_model, text_model, encoders, text_encoders):
    # Predict audio
    audio_pred = predict_audio(audio_path, audio_model, encoders)
    audio_emotion = encoders['emotion'].inverse_transform([np.argmax(audio_pred)])[0]

    # Predict text
    text_label, text_pred = predict_text(text, text_model, text_encoders['tokenizer'], text_encoders['label_encoder'])

    # Optionally, combine predictions
    # For example: average scores
    final_pred = (audio_pred + text_pred) / 2
    final_emotion = encoders['emotion'].inverse_transform([np.argmax(final_pred)])[0]

    return {
        "audio_emotion": audio_emotion,
        "text_emotion": text_label,
        "final_emotion": final_emotion
    }